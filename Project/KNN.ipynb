{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0af57a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# part 1\n",
    "# 1.a: load data frame from file, store into df\n",
    "df = pd.read_csv('KNNAlgorithmDataset.csv')\n",
    "#print(df, type(df))\n",
    "\n",
    "#1.c: shuffle the samples\n",
    "df = df.sample(frac=1)  #??? q01: if re-indexing needed? \n",
    "# frac = [0,1], incase frac = 0.5, shuffle 50% of the rows\n",
    "#print(df)\n",
    "\n",
    "# convert data frame into array\n",
    "data_array = df.values[:, 1:]  #unselect 1st column / label y\n",
    "#print(data_array)\n",
    "\n",
    "# 1.b: set column 'diagnosis' as target variable y / the label\n",
    "y = df[\"diagnosis\"].values\n",
    "#print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c20cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.097e+01 1.122e+01 2.161e+01 ... 1.919e+01 1.234e+01 1.290e+01]\n",
      " [1.720e+01 1.986e+01 2.228e+01 ... 1.594e+01 2.222e+01 1.592e+01]\n",
      " [7.173e+01 7.194e+01 1.444e+02 ... 1.263e+02 7.985e+01 8.374e+01]\n",
      " ...\n",
      " [1.555e-01 2.022e-02 2.422e-01 ... 1.777e-01 8.194e-02 1.012e-01]\n",
      " [2.540e-01 3.292e-01 3.828e-01 ... 2.443e-01 2.268e-01 3.549e-01]\n",
      " [9.532e-02 6.522e-02 1.007e-01 ... 6.251e-02 9.082e-02 8.118e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Task 01: normalizing the data with z-score normalization\n",
    "\n",
    "mean = data_array.mean(axis=0)\n",
    "standard_deviation = data_array.std(axis=0)\n",
    "\n",
    "normalized_data = (data_array - mean) / standard_deviation\n",
    "\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d494e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 01: normalizing the data with z-score normalization\n",
    "# QN why does data shift 1st column by 1, the rest by 2?\n",
    "\n",
    "\n",
    "for i in df:\n",
    "    feature = df[i].values\n",
    "    feature_mean = feature.mean()\n",
    "    feature_standard_deviation = feature.std()  \n",
    "    print(feature)\n",
    "\n",
    "    for x in feature:\n",
    "        x = ((x - feature_mean) / feature_standard_deviation)\n",
    "\n",
    "    #z_Score_Normalization(df[feature])\n",
    "\n",
    "array = df[\"perimeter_mean\"].values\n",
    "#z_Score_Normalization(array)\n",
    "#print(array)\n",
    "#print(df[\"perimeter_mean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "492f8a27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Task 02: seperate data into training(70%), validation(20%) and test(10%) sets by slicing\n",
    "da_len = len(data_array)   #number of rows / length of data_array\n",
    "training_set = data_array[:int(da_len * 0.7)]\n",
    "#print(training_set)\n",
    "validation_set = data_array[int(da_len * 0.7): int(da_len * 0.9)]\n",
    "test_set = data_array[int(da_len* 0.9):]\n",
    "#print(len(training_set) + len(validation_set) + len(test_set) == da_len) # the whole data array included in the 3 sets: True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d530e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.c distance algorithms with input of two 1D-Vectors, return distance (real number) of 2 vectors\n",
    "import numpy as np\n",
    "# Manhattan distance\n",
    "def manhattan_distance(vector01, vector02):\n",
    "    # raise error if data points are not of the same length\n",
    "    if (len(vector01) != len(vector02)):\n",
    "        raise ValueError(\"Unequal length of inputed data points\")\n",
    "    # return sum(\\xi_m - xj_m\\)\n",
    "    return np.sum(np.abs(vector01 - vector02))    # np.abs(): absolute value of each elements\n",
    "    # a variante: use np.fabs(v1, v2), return np.sum(np.fabs(vector01 - vector02))\n",
    "\n",
    "# Euclidean distance\n",
    "def euclidean_distance(vector01, vector02):\n",
    "    if (len(vector01) != len(vector02)):\n",
    "        raise ValueError(\"Unequal length of inputed data points\")\n",
    "    # use np library: calculate the norm of the new vector (difference of two old vectors)\n",
    "    return np.linalg.norm(vector01 - vector02)\n",
    "\n",
    "# Chebyshev distance\n",
    "def chebyshev_distance(vector01, vector02):\n",
    "    if (len(vector01) != len(vector02)):\n",
    "        raise ValueError(\"Unequal length of inputed data points\")\n",
    "    # return max(\\xi_01 - xi_02\\)\n",
    "    absolute_array = [np.abs(vector01[i] - vector02[i]) for i in range(len(vector01))]\n",
    "    return max(absolute_array)\n",
    "\n",
    "\n",
    "# test algo with data: \n",
    "#t_p01 = training_set\n",
    "#t_p02 = training_set\n",
    "#t_short01 = [1.1,2,4.43,8]\n",
    "#t_short02 = [3,4.7,6,9.69]\n",
    "\n",
    "#print(manhattan_distance(t_p01, t_p02))    # test (compared with result by online calculator): True\n",
    "#print(euclidean_distance(t_p01, t_p02))    # definition from numpy library\n",
    "#print(chebyshev_distance(t_short01, t_short02))    # test: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4fbc2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: implement the kNN algorithm\n",
    "# accroding to 2.2 kNN algo is capsuled in a kNN predictor\n",
    "class kNN_predictor:\n",
    "    #constructor: define attribiutes\n",
    "    def __init__(self, k, distance_func):    #q: if the k and dist-func is given as param in constructor or later in predict func?\n",
    "        self.trained = False   #default: not trained\n",
    "        self.k = k  # k neighbours\n",
    "        self.d = distance_func\n",
    "        # X(data set) and Y(classes / target values) will be provided in fit() as parameters\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        # something about the classification / regression calculated based on data from training set\n",
    "        \n",
    "    # functions\n",
    "        def fit(self, training_X, training_Y):\n",
    "            #feed the model with training data\n",
    "            self.X = training_X\n",
    "            self.Y = training_Y\n",
    "            #do something\n",
    "            self.trained = True   #set trained\n",
    "            \n",
    "        def predict(self):\n",
    "            #predict classification or regression result for new data.\n",
    "            # check if predictor is trained\n",
    "            if self.trained == False:\n",
    "                raise(\"Predictor is not trained! Train it first by calling fit()!\")\n",
    "            # TODO: call normalize() to normalize the data\n",
    "            return None\n",
    "            \n",
    "        def confusionMatrix(self, X):\n",
    "            #compute confusion matrix, input: X (user input)\n",
    "            return None\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3184e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 3.2: check if user inputs are valid\n",
    "def check_predictor_input(k, d):\n",
    "    valid_dist_algos = ['manhattan', 'euclidean', 'chebyshev']\n",
    "    # TODO: not sure about the k value area, len(training_set) = 398\n",
    "    if k > len(training_set):\n",
    "        raise ValueError(f\"Given k too large! Give a number not larger than {len(training_set)}\")\n",
    "    if d.lower() not in valid_dist_algos:\n",
    "        raise ValueError(f\"Given algorithm is not valid, please choose from {valid_dist_algos}!\")\n",
    "        \n",
    "#test: passed\n",
    "#check_predictor_input(100, 'manha')\n",
    "#check_predictor_input(400, 'chebyshev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b5545ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 3.3: make an API\n",
    "class simple_API:\n",
    "    def __init__(self, k, d):\n",
    "        # check if k and d are valid\n",
    "        check_predictor_input(k, d)\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        \n",
    "        # create predictor with inputs\n",
    "        self.predictor = kNN_predictor(k, d)\n",
    "        \n",
    "        # train the predictor with training data \n",
    "        x = training_set\n",
    "        y = y[:len(training_set)]   #slice the labels for training set\n",
    "        # call predictor.fit() with training data\n",
    "        self.predictor.fit(x, y)\n",
    "   \n",
    "    #predict new data\n",
    "    def predict(self, newdata):\n",
    "        return self.predictor.predict(#something)\n",
    "            \n",
    "    #a func make graphic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: test the data with user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f887c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
